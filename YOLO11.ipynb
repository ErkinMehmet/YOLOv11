{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e4310f-aa92-482c-a437-1081544459fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet ultralytics\n",
    "# 1. Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79002a6a-f88b-44e1-9954-f363e9d0becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\hli\\OneDrive - FQM\\Bureau\\learning\\mlops\\YOLOv11\\static\\images\\test.jpg: 448x640 9 persons, 1 tie, 129.4ms\n",
      "Speed: 6.4ms preprocess, 129.4ms inference, 13.3ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# detection\n",
    "from ultralytics import YOLO\n",
    "# load a model\n",
    "model=YOLO(\"yolo11n.pt\")\n",
    "# perform object detection on an image\n",
    "results=model(\"static/images/test.jpg\")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408e6ab0-825c-4ad2-af27-cb4ae3709d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncap = cv2.VideoCapture(0)\\nwhile True:\\n    ret, frame = cap.read()\\n    if not ret:\\n        breakq\\n    # YOLO attend des images BGR (comme OpenCV)\\n    results = model(frame)  # retourne les détections\\n    # Dessiner les boîtes sur l\\'image\\n    annotated_frame = results[0].plot()  # plot() retourne l\\'image annotée\\n    # Afficher l\\'image\\n    cv2.imshow(\"YOLO Webcam\", annotated_frame)\\n    # Quitter avec la touche \\'q\\'\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n    break # to quit immediately\\n# Libérer la caméra et fermer les fenêtres\\ncap.release()\\ncv2.destroyAllWindows()\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "# for video\n",
    "# results = model(\"vid.mp4\", save=True)\n",
    "\n",
    "# webcam\n",
    "\"\"\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        breakq\n",
    "    # YOLO attend des images BGR (comme OpenCV)\n",
    "    results = model(frame)  # retourne les détections\n",
    "    # Dessiner les boîtes sur l'image\n",
    "    annotated_frame = results[0].plot()  # plot() retourne l'image annotée\n",
    "    # Afficher l'image\n",
    "    cv2.imshow(\"YOLO Webcam\", annotated_frame)\n",
    "    # Quitter avec la touche 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    break # to quit immediately\n",
    "# Libérer la caméra et fermer les fenêtres\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057f2d28-e397-4620-a858-b535591eda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\hli\\OneDrive - FQM\\Bureau\\learning\\mlops\\YOLOv11\\static\\images\\test.jpg: 448x640 9 persons, 1 tie, 56.6ms\n",
      "Speed: 6.4ms preprocess, 56.6ms inference, 116.2ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# segmentation\n",
    "model2=YOLO(\"yolo11n-seg.pt\")\n",
    "results2=model(\"static/images/test.jpg\")\n",
    "results2[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79cd120e-37b0-4e2d-870c-52a49a2c719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\hli\\OneDrive - FQM\\Bureau\\learning\\mlops\\YOLOv11\\static\\images\\test.jpg: 448x640 9 persons, 1 tie, 132.0ms\n",
      "Speed: 4.2ms preprocess, 132.0ms inference, 116.2ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# pose estimation\n",
    "model3=YOLO(\"yolo11n-pose.pt\")\n",
    "results3=model(\"static/images/test.jpg\")\n",
    "results3[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91214775-852b-4c1b-8cc7-9d991b175d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "# model4=YOLO(\"yolo11n-cls.pt\")\n",
    "# results4=model(\"static/images/test.png\",save=True) # perform object detection on an image\n",
    "# results4[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb19b29-7dbc-4271-a282-d93332350d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\hli\\OneDrive - FQM\\Bureau\\learning\\mlops\\YOLOv11\\static\\images\\test.jpg: 448x640 9 persons, 1 tie, 100.0ms\n",
      "Speed: 14.5ms preprocess, 100.0ms inference, 86.6ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# oriented bouding box\n",
    "model5=YOLO(\"yolo11n-obb.pt\")\n",
    "results5=model(\"static/images/test.jpg\") # perform object detection on an image\n",
    "results5[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91f72a4-4a97-456c-aaf6-92687f985ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 08:32:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 573.57                 Driver Version: 573.57         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 2000 Ada Gene...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   35C    P8              3W /   60W |     983MiB /   8188MiB |     50%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1880    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A            7028    C+G   ...mith\\Snagit 2019\\Snagit32.exe      N/A      |\n",
      "|    0   N/A  N/A            8032    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A           15084    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           18640    C+G   ...\\Snagit 2019\\SnagitEditor.exe      N/A      |\n",
      "|    0   N/A  N/A           18988    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A           20148    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           21076    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           25844    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A           27900    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A           29300    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A           30572    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A           37932    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A           46400    C+G   ...ffice\\root\\Office16\\EXCEL.EXE      N/A      |\n",
      "|    0   N/A  N/A           47200    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A           52540    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A           58572    C+G   ....0.3595.94\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           65984    C+G   ....0.3595.94\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           83092    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A           89276    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "|    0   N/A  N/A           93716    C+G   ...yb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "|    0   N/A  N/A           98972      C   ...am Files\\Python310\\python.exe      N/A      |\n",
      "|    0   N/A  N/A          103552    C+G   ...munity\\Common7\\IDE\\devenv.exe      N/A      |\n",
      "|    0   N/A  N/A          111884    C+G   ...iceHub.ThreadedWaitDialog.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 2. custom training for OD\n",
    "# check gpu\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37bf332-a2a2-4457-a661-be1c882e6c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.235  Python-3.10.0 torch-2.9.1+cu128 CUDA:0 (NVIDIA RTX 2000 Ada Generation Laptop GPU, 8188MiB)\n",
      "Setup complete  (32 CPUs, 31.7 GB RAM, 429.9/475.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de867b6-2ae3-429d-9529-2845f63ff533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hli\\\\OneDrive - FQM\\\\Bureau\\\\learning\\\\mlops\\\\YOLOv11\\\\PPE_Detection-1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "ROBOFLOW_TOKEN=os.getenv(\"ROBOFLOW_TOKEN\")\n",
    "# automatically download data\n",
    "# a folder will be downloade from robo flow containing test, train, valid folders + 2 READMEs + data.yaml containing the basic info\n",
    "rf = Roboflow(api_key=ROBOFLOW_TOKEN)\n",
    "project = rf.workspace(\"project-uyrxf\").project(\"ppe_detection-v1x3l\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov11\")\n",
    "\n",
    "dataset.location \n",
    "# we need to manually change the train, val, test paths in the data.yaml file according to dataset.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957feb5-c6a1-42ad-b832-3b66f64db077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train yolo11 model on the custom dataset\n",
    "!yolo task=detect mode=train data=\"{dataset.location}/data.yaml\" model=\"yolo11n.pt\" epochs=50 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34571ea6-ff6b-419c-aa4b-a451ec546b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine training results\n",
    "def checkResults(task=\"detect\",mode=\"train\"):\n",
    "    Image(f\"/content/runs/{task}/{mode}/confusion_matrix.png\",width=600)\n",
    "    Image(f\"/content/runs/{task}/{mode}/results.png\",width=600) # all the metrics\n",
    "    Image(f\"/content/runs/{task}/{mode}/train_batch0.jpg\",width=600) # images\n",
    "    Image(f\"/content/runs/{task}/{mode}/train_batch0_pred.jpg\",width=600)\n",
    "    Image(f\"/content/runs/{task}/{mode}/train_batch1_pred.jpg\",width=600)\n",
    "checkResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a9996-d660-489c-9b30-944e9dc91329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation the fine-tuned model\n",
    "!yolo task=detect mode=val model=\"content/runs/detect/train/weights/best.pt\" data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815adaa-cde6-4b2f-8cd6-12ac7a9395f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with cusotm model on images\n",
    "!yolo task=detect mode=predict  model=\"content/runs/detect/train/weights/best.pt\" conf=0.25 source={dataset.location}/test/images save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f8563-09e1-4e28-b5b1-e8127513683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some of the predictions\n",
    "import glob,os\n",
    "from IPython.display import Image as IPyImage,display\n",
    "latest_folder=max(glob.glob('/content/runs/detect/predict*/'),key=os.path.getmtime)\n",
    "for img in glob.glob(f'{latest_folder}/*.jpg')[1:4]:\n",
    "    display(IPyImage(filename=img,width=600))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0dc6d-a7a4-473d-94d1-40d8542d3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download one data image\n",
    "!gdown \"https://drive.google.com/uc?id=1ACZp9gmtjEdIY-SHp8K0xdR0D8Gjd3j3&confirm=t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917a638-db81-42f2-9ec8-a7890a64952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=\"content/runs/detect/train/weights/best.pt\" conf=0.25 source=image2.jpg save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb5858-e5ff-4af0-89b6-626abea7c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/content/runs/detect/predict2/image2.jpg\",width=600) # show the predicted image with bounding boxes and predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fea532-7ae0-47bb-ad92-e43724d54ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on videos\n",
    "#!gdown \"https://drive.google.com/uc?id=1Nyl18zr7zOJHh8uHtSic3dsJSY4LI_t-&confirm=t\"\n",
    "#!yolo task=detect mode=predict model= \"/content/runs/detect/train/weights/best.pt\" conf=0.25 source=\"PPE_Part1.mp4\" save=True\n",
    "# from IPython.display import HTML\n",
    "#from base64 import b64encode\n",
    "#import os\n",
    "#save_path='/content/runs/detect/predict3/PPE_Part1.avi'\n",
    "#compressed_path= \"/content/result_compressed.mp4\"\n",
    "#os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\") # to compress the vieo\n",
    "#mp4=open(compressed_path,'rb').read()\n",
    "#data_url=\"data:video/mp4;base64,\"+b64encode(mp4).decode()\n",
    "#HTML(\"\"\"\n",
    "#\"\"\" % data_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7cf9c-1728-4bfb-86de-2582f0a87d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data segmentation\n",
    "rf = Roboflow(api_key=ROBOFLOW_TOKEN)\n",
    "project = rf.workspace(\"iotseecs\").project(\"brain-tumor-yzzav\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov11\")\n",
    "\n",
    "# load the model\n",
    "model2=YOLO(\"yolo11n-seg.pt\")\n",
    "# train\n",
    "train_results2=model2.train(\n",
    "    data=\"/content/BRAIN-TUMOR-1/data.yaml\" # path to data\n",
    "    , epochs=10, imsz=640,device=0) # number of training epochs, training image size, device to run on, i.e. device = 0,1,2,3 or device=cpu\n",
    "\n",
    "# inference\n",
    "checkResults(\"segment\")\n",
    "\n",
    "best_model2=YOLO('/content/runs/segment/train/weights/best.pt')\n",
    "best_results21=model(\"/content/1.jpg\",save=True)\n",
    "best_results21[0].show(0)\n",
    "best_results22=model(\"/content/3.jpg\",save=True)\n",
    "best_results22[0].show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5080d55-4905-4bca-a934-103c6eca0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# image classification\n",
    "model3 = YOLO('yolo11n-cls.pt')\n",
    "results3 = model.train(data='/content/Chicken-fecal-images', epochs=20, imgsz=640)\n",
    "Image(\"/content/runs/classify/train/results.png\", width=600)\n",
    "\n",
    "model = YOLO('/content/runs/classify/train/weights/best.pt')\n",
    "# inference\n",
    "results = model(\"/content/cocci.0.jpg\") #, save=True, conf=0.5)\n",
    "results[0].show()\n",
    "results = model(\"/content/healthy.0.jpg\", save=True, conf=0.5)\n",
    "results[0].show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3fbcd8-bc21-437c-8ba1-ced3d66f7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Image Segmentation YOLOv11 for OD + SAM2 for OS\n",
    "# if we do not want to annotate the images\n",
    "rf = Roboflow(api_key=ROBOFLOW_TOKEN)\n",
    "project = rf.workspace(\"brain-tumor-detection-wsera\").project(\"tumor-detection-ko5jp\")\n",
    "version = project.version(8)\n",
    "dataset = version.download(\"yolov11\")\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"/content/Tumor-Detection-8/data.yaml\",  # path to dataset YAML\n",
    "    epochs=20,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=0,  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    ")\n",
    "# inference\n",
    "Image(\"/content/runs/detect/train/results.png\", width=600)\n",
    "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(\"/content/meningioma_3.jpg\", save=True)\n",
    "results[0].show()\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    print(boxes)\n",
    "\n",
    "from ultralytics import SAM\n",
    "yolo_model=YOLO(\"/content/runs/detect/train/weights/best.pt\") # load the yolo model for OD\n",
    "# run batched inference on a list of images\n",
    "results=yolo_model(\"/content/meningioma_3.jpg\") # return a list of results objects\n",
    "# load SAM\n",
    "sam_model=SAM(\"sam2_b.pt\")\n",
    "for r in results:\n",
    "    class_ids=r.boxes.cls.int().tolist() # noqa\n",
    "    if len(class_ids):\n",
    "        boxes=r.boxes.xyxy # boxes object for bbox outputs\n",
    "        same_results=sam_model(r.orig_img,bboxes=boxes,verbose=False,save=True,device=0) # runs/segment/predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a8b2f-bc09-4ddb-9b2b-f8c56583765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Tracking with YOLOv11, Byte_Track and Bot_SORT\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11m.pt\")\n",
    "\n",
    "# Perform object detection on video\n",
    "results = model(\"/content/p.mp4\", save=True) # detect all classes\n",
    "results = model(\"/content/p.mp4\",classes=0, save=True) # if we only want to detect one class\n",
    "\n",
    "# tracking\n",
    "# bot sort\n",
    "results=model.track(source=\"/content/p.mp4\",persist=True, # keep the same IDs between sessions if track is executed again\n",
    "                    save=True)\n",
    "# byte track\n",
    "results=model.track(source=\"/content/p.mp4\",persist=True,show=True,classes=0,tracker=\"bytetrack.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a96b42-97a1-4ced-8d3b-a655d4fcc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign language detection\n",
    "# download dataset\n",
    "import gdown\n",
    "# download the data from g drive\n",
    "url = \"https://drive.google.com/file/d/1CWSwE0hQOl05c1B9f2SM4j5cjohxZ6Fm/view?usp=sharing\"\n",
    "file_id = url.split(\"/\")[-2]\n",
    "print(file_id)\n",
    "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "gdown.download(prefix+file_id, \"sign_data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a13ae-8eae-4dd8-877c-b1243e8b34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip sign_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de664b9-d162-4611-8bbf-2b3db15452f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "!yolo task=detect mode=train data=\"/content/data.yaml\" model=\"yolo11n.pt\" epochs=50 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c3b95-0fff-4354-97f9-4689bd429582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine\n",
    "Image(\"/content/runs/detect/train/confusion_matrix.png\", width=600)\n",
    "Image(\"/content/runs/detect/train/labels.jpg\", width=600)\n",
    "Image(\"/content/runs/detect/train/results.png\", width=600)\n",
    "Image(\"/content/runs/detect/train/train_batch0.jpg\", width=600)\n",
    "Image(\"/content/runs/detect/train/val_batch0_pred.jpg\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88849c97-5536-4c42-840a-cedf7c07faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "!yolo task=detect mode=predict model=\"/content/runs/detect/train/weights/best.pt\" conf=0.25 source=\"/content/test/images\" save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328e778-5094-4680-bebb-9153f75d01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_folder = max(glob.glob('/content/runs/detect/predict*/'), key=os.path.getmtime)\n",
    "for img in glob.glob(f'{latest_folder}/*.jpg')[1:4]:\n",
    "    display(IPyImage(filename=img, width=600))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dff24-51d8-415b-8286-d571cbb8ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model= \"/content/runs/detect/train/weights/best.pt\" conf=0.25 source=\"/content/hello.jpg\" save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8b400-a182-4315-a48c-40e3e64a1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/content/runs/detect/predict2/hello.jpg\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3a5bb-f1e7-4665-beb8-8660c85db5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with custom model on videos\n",
    "!yolo task=detect mode=predict model= \"/content/runs/detect/train/weights/best.pt\" conf=0.25 source=\"PPE_Part1.mp4\" save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90193d-fd81-4254-9206-8b3be7b1e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video path\n",
    "save_path = '/content/runs/detect/predict3/PPE_Part1.avi'\n",
    "\n",
    "# Compressed video path\n",
    "compressed_path = \"/content/result_compressed.mp4\"\n",
    "\n",
    "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
    "\n",
    "# Show video\n",
    "mp4 = open(compressed_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac8f52-1282-4e24-9de4-297431ff0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helmet detection\n",
    "# load data\n",
    "rf = Roboflow(api_key=ROBOFLOW_TOKEN)\n",
    "project = rf.workspace(\"yolo-do-it-yhopz\").project(\"helmet-detector-9rzmg-bmd6q\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae416b-ea66-429a-9d48-ec23e93f765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "!yolo task=detect mode=train data={dataset.location}/data.yaml model=\"yolo11n.pt\" epochs=50 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ebfb9-6ff7-455c-9eb7-9142f4bb3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine\n",
    "Image(\"/content/runs/detect/train/confusion_matrix.png\", width=600)\n",
    "Image(\"/content/runs/detect/train/labels.jpg\", width=600)\n",
    "Image(\"/content/runs/detect/train/results.png\", width=600)\n",
    "Image(\"/content/runs/detect/train/train_batch0.jpg\", width=600)\n",
    "Image(\"/content/runs/detect/train/val_batch0_pred.jpg\", width=600)\n",
    "Image(\"/content/runs/detect/train/val_batch1_pred.jpg\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e31922-988b-4563-8e4e-211f592f9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "!yolo task=detect mode=predict model=\"/content/runs/detect/train/weights/best.pt\" conf=0.25 source={dataset.location}/test/images save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3215d-e504-43c0-aaad-e39ba3fb942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_folder = max(glob.glob('/content/runs/detect/predict*/'), key=os.path.getmtime)\n",
    "for img in glob.glob(f'{latest_folder}/*.jpg')[1:4]:\n",
    "    display(IPyImage(filename=img, width=600))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468424f-f530-49c5-abdc-1b2f6c2d0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model= \"/content/runs/detect/train/weights/best.pt\" conf=0.25 source=helmet.jpg save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46d044-f5de-4be6-aba2-d03b86337125",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"/content/runs/detect/predict2/helmet.jpg\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f82eb8-11e3-4e93-a73c-60d6907c51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on video\n",
    "!yolo task=detect mode=predict model= \"/content/runs/detect/train/weights/best.pt\" conf=0.25 source=\"PPE_Part1.mp4\" save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecf916-4c11-406c-9836-2532e93f3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video path\n",
    "save_path = '/content/runs/detect/predict3/PPE_Part1.avi'\n",
    "# Compressed video path\n",
    "compressed_path = \"/content/result_compressed.mp4\"\n",
    "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
    "# Show video\n",
    "mp4 = open(compressed_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677a16c-8db6-4e8c-8d80-b1945d93b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto data annotation with SAM\n",
    "from ultralytics.vit import SAM\n",
    "model = SAM('sam_b.pt')\n",
    "model.predict('images/dog.jpeg')\n",
    "results = model.predict('video/kids.mp4', device = 0) # works with a video\n",
    "results = model.predict(0, device = 0, show = True) # webcam\n",
    "\n",
    "# generate segmentation dataset using a detection model\n",
    "from ultralytics.yolo.data.annotator import auto_annotate\n",
    "auto_annotate(data=\"images\",det_model=\"yolov8x.pt\",sam_model='sam_b.pt')\n",
    "\n",
    "# code for auto annotation\n",
    "from ultralytics.vit.sam import PromptPredictor,build_sam\n",
    "from ultralytics.yolo.utils.torch_utils import select_device\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "def auto_connotate(data,det_model='yolov8x.pt',sam_model='sam_b.pt',device='',output_dir=None):\n",
    "     \"\"\"\n",
    "    Automatically annotates images using a YOLO object detection model and a SAM segmentation model.\n",
    "    Args:\n",
    "        data (str): Path to a folder containing images to be annotated.\n",
    "        det_model (str, optional): Pre-trained YOLO detection model. Defaults to 'yolov8x.pt'.\n",
    "        sam_model (str, optional): Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'.\n",
    "        device (str, optional): Device to run the models on. Defaults to an empty string (CPU or GPU, if available).\n",
    "        output_dir (str, None, optional): Directory to save the annotated results.\n",
    "            Defaults to a 'labels' folder in the same directory as 'data'.\n",
    "    \"\"\"\n",
    "    device=select_device(device)\n",
    "    det_model=YOLO(det_model)\n",
    "    sam_model=build_sam(sam_model)\n",
    "    det_model.to(device)\n",
    "    sam_model.to(device)\n",
    "    if not output_dir:\n",
    "        output_dir=Path(str(data)).parent/'labels'\n",
    "    Path(output_dir).mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    prompt_predictor=PromptPredictor(sam_model) # create a predictor SAM that can do inferences \n",
    "    det_results=det_model(data,stream=True) # run YOLO detection on all images\n",
    "    for results in del_results: # for each image treated by YOLO, extract bounding box and label\n",
    "        boxes=result.boxes.xyxy # boxes object for bbox outputs\n",
    "        class_ids=result.boxes.cls.int().tolist() # noqa \n",
    "        if len(class_ids):\n",
    "            prompt_predictor.set_image(result.orig_img)\n",
    "            mask,_,_ = prompt_predictor.predict_torch(\n",
    "                point_coords=None,point_labels=None,\n",
    "                boxes=prompt_predictor.transform.apply_boxes_torch(boxes,result.orig_shape[:2],\n",
    "                multimask_output=False,) # sam predicts the mask for each object\n",
    "            result.update(masks=masks.sequeeze(1)) # insert the mask\n",
    "            segments=result.masks.xyn # SAM does more detailed segmentation and adds info to yolo\n",
    "            with open (str(Path(output_dir)/Path(result.path).stem) +'.txt.','w') as f:\n",
    "                for i in range(len(segments)):\n",
    "                    s=segments[i]\n",
    "                    if len(s)==0:\n",
    "                        continue\n",
    "                    segment=map(str,segments[i].reshape(-1).tolist())\n",
    "                    f.write(f'{class_ids[i]} ' + ' '.join(segment) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976f322-6a72-462c-995a-4f8c13361bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
